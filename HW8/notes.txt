Example 1) (give them the example, have them work out in small groups
what the complexity should be, give them a bit of time, and then gather
estimates). We're looking for worst-case complexity

 - Reverse the linked list (this returns the pointer to the head of the
reversed list).

 - Don't give them an algorithm :) let them figure it out - pseudocode is
enough, hopefully we'll find at least these two:

    a)
    * Create a temporary list initially empty
    * While head!=NULL
          make a copy of the node at head (head_copy)
          insert head_copy into temporary list, at head
          delete head from input list (which will either update the
           head to the next node, or return NULL if the list is empty)
    * Return new list head

    - This should be O(N) - there are N entries in the list, for each of
      them we do one copy, one insert at head

    b)
    * Create a temporary list initially empty
    * While head!=NULL
          Find tail of list
          Remove (unlink) the tail node
          insert the tail node into the temporary list at the tail
    * Return new list head

    - This is bad, we are doing an O(N) traversal for each node, TWICE, once
      to find the tail node, once to insert it at the tail of the temporary
      list. So, O(N^2)

    - But it doesn't make copies of node data

    If only one of these options was suggested by students, discuss the
other one! make sure they are clear on how they work, and then ask
them:

    - Which one do we use if
       * Data collection is very large? why?
         (should be straightforward, a) beast b) anyday!)
       * Data collection is not huge (e.g. a couple hundred entries) and
         nodes contain lots of data (e.g. videos, music, images, etc.)
         (not so straightforward, real answer is 'it depends', but
          hopefully someone brings up the cost of making copies of nodes,
          space utilization for a) is twice that of b) ).

     - Remind them that which algorithm is most suitable for some
situation depends on the situation :) they need to sit down and work
out complexity and then figure out based on how large their N is, and
how much data is in each node, what is the optimal solution.

 Example 2) (same procedure as example 1)

 Worst case for detecting and removing duplicate nodes from the linked list:

 Have then work out the algorithm (pseudocode is enough), here's a
possible solution:

  p=head
  while p!=NULL
     q=head->next
      while (q!=NULL)
         if contents of p == contents of q
             delete(q)
             q=q->next
     p=p->next

  This is doing an O(N) traversal over the list for value of p, so it is
either O(N^2) just for the traversal part.

  Ask them if this is the correct complexity value, if no one has
objections, ask them 'what about the cost of delete(q)'?

  If handled smartly (the traversal on q keeps track of the predecessor,
or we compare the contents of q->next instead of q), then it's O(1) to
delete(q) so we get O(N^2) for the whole thing.

  However, if delete(q) has to do a traversal, then we're in trouble
because we're doing another O(N) traversal to delete the node! UGH.
O(N^3), not nice.

 So encourage them to think about where there are 'bottlenecks', places in
the algorithm that have a high computational cost, and ask them to keep
in mind these are the places you look to first if you want to write a
more efficient algorithm.

  If you have time, have some fun asking them what's the BEST CASE
complexity for the above algorithm - send me your estimate and a brief
explanation why you think that is ;)